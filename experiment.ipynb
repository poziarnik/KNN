{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/projects/KNN/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BASE_DATA_DIR = Path(\"./data\")\n",
    "BASE_DATA_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "Download BUT-LCC dataset. \n",
    "\n",
    "### Format\n",
    "```json\n",
    "{\n",
    "  \"id\": unique identifier, \t\t\n",
    "  \"part\": original source, \n",
    "  \"title\": source document title, \n",
    "  \"text\": the context,\n",
    "  \"ugly\": bool,\n",
    "  \"ugly_score\": float\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"BUT-FIT/BUT-LCC\"\n",
    "FILE_NAME = \"train_0.jsonl.gz\"\n",
    "\n",
    "dataset_path = hf_hub_download(repo_id=REPO_ID, filename=FILE_NAME, repo_type=\"dataset\")\n",
    "dataset = load_dataset('json', data_files=dataset_path, split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and save dataset \n",
    "- Filter data only from czech wikipedia.\n",
    "- Saving dataset (checkpoint) for speeding things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"part\"] == \"cswiki-20230101\")\n",
    "dataset.save_to_disk(BASE_DATA_DIR / \"cs-wiki\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(BASE_DATA_DIR / \"cs-wiki\")\n",
    "texts = dataset['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentences\n",
    "- clear sentences of any non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences from text\n",
    "sentences = []\n",
    "for i, text in enumerate(texts):\n",
    "    sentences.extend(text.split('.'))\n",
    "    if i == 1000:\n",
    "        break\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    # Remove non-alphabetic characters and extra spaces\n",
    "    text = re.sub(r'[^a-zA-Zá-žÁ-Ž ]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "cleaned_sentences = [clean_text(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce errors\n",
    "- add errors into the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to introduce errors in a word\n",
    "def introduce_errors(word):\n",
    "    if len(word) > 1:\n",
    "        index = random.randint(0, len(word) - 1)\n",
    "        return word[:index] + random.choice(string.ascii_lowercase) + word[index+1:]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# Function to generate error-introduced sentences\n",
    "def generate_error_sentences(sentences, error_rate=0.1):\n",
    "    error_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        error_words = [introduce_errors(word) if random.random() < error_rate else word for word in words]\n",
    "        error_sentence = ' '.join(error_words)\n",
    "        error_sentences.append(error_sentence)\n",
    "    return error_sentences\n",
    "\n",
    "# Example usage:\n",
    "error_rate = 0.1  # Adjust error rate as needed\n",
    "error_introduced_sentences = generate_error_sentences(cleaned_sentences, error_rate)\n",
    "\n",
    "\n",
    "# Split data into train and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(error_introduced_sentences, cleaned_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert lists to dictionaries\n",
    "train_data = {\"sentence\": x_train, \"labels\": y_train}\n",
    "val_data = {\"sentence\": x_val, \"labels\": y_val}\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer\n",
    "We need to convert our text into numerical data to feed into the model. We need to make sure that for every input character in data there is corresponding numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train_from_iterator(texts, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_model(\".\", \"example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data\n",
    "- load saved tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Get the sentence and its label\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[0;32m---> 13\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Tokenize the sentence and its label\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     tokenized_sentence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(sentence)\n",
      "File \u001b[0;32m~/projects/KNN/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2808\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m \u001b[38;5;129m@overload\u001b[39m\n\u001b[1;32m   2805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2806\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m-> 2808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m   2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(key)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"example-vocab.json\",\n",
    "    \"example-merges.txt\"\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(example: dict[str, str]):\n",
    "    encodings = tokenizer.encode_batch(example[\"sentence\"])\n",
    "\n",
    "    print(encodings)\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize, batched=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
